{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import Normal\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "# for CI testing\n",
    "smoke_test = ('CI' in os.environ)\n",
    "pyro.enable_validation(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100  # size of toy data\n",
    "\n",
    "def build_linear_dataset(N, p=1, noise_std=0.01):\n",
    "    X = np.random.rand(N, p)\n",
    "    # w = 3\n",
    "    w = 3 * np.ones(p)\n",
    "    # b = 1\n",
    "    y = np.matmul(X, w) + np.repeat(1, N) + np.random.normal(0, noise_std, size=N)\n",
    "    y = y.reshape(N, 1)\n",
    "    X, y = torch.tensor(X).type(torch.Tensor), torch.tensor(y).type(torch.Tensor)\n",
    "    data = torch.cat((X, y), 1)\n",
    "    assert data.shape == (N, p + 1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        # p = number of features\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(p, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "regression_model = RegressionModel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/udion/anaconda3/envs/041_36/lib/python3.7/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0050] loss: 30.7408\n",
      "[iteration 0100] loss: 4.0721\n",
      "[iteration 0150] loss: 2.3983\n",
      "[iteration 0200] loss: 1.3171\n",
      "[iteration 0250] loss: 0.6594\n",
      "[iteration 0300] loss: 0.3043\n",
      "[iteration 0350] loss: 0.1318\n",
      "[iteration 0400] loss: 0.0560\n",
      "[iteration 0450] loss: 0.0257\n",
      "[iteration 0500] loss: 0.0146\n",
      "[iteration 0550] loss: 0.0109\n",
      "[iteration 0600] loss: 0.0098\n",
      "[iteration 0650] loss: 0.0095\n",
      "[iteration 0700] loss: 0.0094\n",
      "[iteration 0750] loss: 0.0094\n",
      "[iteration 0800] loss: 0.0094\n",
      "[iteration 0850] loss: 0.0094\n",
      "[iteration 0900] loss: 0.0094\n",
      "[iteration 0950] loss: 0.0094\n",
      "[iteration 1000] loss: 0.0094\n",
      "Learned parameters:\n",
      "linear.weight: 3.000\n",
      "linear.bias: 1.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "optim = torch.optim.Adam(regression_model.parameters(), lr=0.05)\n",
    "num_iterations = 1000 if not smoke_test else 2\n",
    "\n",
    "def main():\n",
    "    data = build_linear_dataset(N)\n",
    "    x_data = data[:, :-1]\n",
    "    y_data = data[:, -1]\n",
    "    for j in range(num_iterations):\n",
    "        # run the model forward on the data\n",
    "        y_pred = regression_model(x_data).squeeze(-1)\n",
    "        # calculate the mse loss\n",
    "        loss = loss_fn(y_pred, y_data)\n",
    "        # initialize gradients to zero\n",
    "        optim.zero_grad()\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        # take a gradient step\n",
    "        optim.step()\n",
    "        if (j + 1) % 50 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss.item()))\n",
    "    # Inspect learned parameters\n",
    "    print(\"Learned parameters:\")\n",
    "    for name, param in regression_model.named_parameters():\n",
    "        print(\"%s: %.3f\" % (name, param.data.numpy()))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    # Create unit normal priors over the parameters\n",
    "    loc, scale = torch.zeros(1, 1), 10 * torch.ones(1, 1)\n",
    "    bias_loc, bias_scale = torch.zeros(1), 10 * torch.ones(1)\n",
    "    w_prior = Normal(loc, scale).independent(1)\n",
    "    b_prior = Normal(bias_loc, bias_scale).independent(1)\n",
    "    priors = {'linear.weight': w_prior, 'linear.bias': b_prior}\n",
    "    # lift module parameters to random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module(\"module\", regression_model, priors)\n",
    "    # sample a regressor (which also samples w and b)\n",
    "    lifted_reg_model = lifted_module()\n",
    "    with pyro.iarange(\"map\", N):\n",
    "        x_data = data[:, :-1]\n",
    "        y_data = data[:, -1]\n",
    "\n",
    "        # run the regressor forward conditioned on data\n",
    "        prediction_mean = lifted_reg_model(x_data).squeeze(-1)\n",
    "        # condition on the observed data\n",
    "        pyro.sample(\"obs\",\n",
    "                    Normal(prediction_mean, 0.1 * torch.ones(data.size(0))),\n",
    "                    obs=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "\n",
    "def guide(data):\n",
    "    # define our variational parameters\n",
    "    w_loc = torch.randn(1, 1)\n",
    "    # note that we initialize our scales to be pretty narrow\n",
    "    w_log_sig = torch.tensor(-3.0 * torch.ones(1, 1) + 0.05 * torch.randn(1, 1))\n",
    "    b_loc = torch.randn(1)\n",
    "    b_log_sig = torch.tensor(-3.0 * torch.ones(1) + 0.05 * torch.randn(1))\n",
    "    # register learnable params in the param store\n",
    "    mw_param = pyro.param(\"guide_mean_weight\", w_loc)\n",
    "    sw_param = softplus(pyro.param(\"guide_log_scale_weight\", w_log_sig))\n",
    "    mb_param = pyro.param(\"guide_mean_bias\", b_loc)\n",
    "    sb_param = softplus(pyro.param(\"guide_log_scale_bias\", b_log_sig))\n",
    "    # guide distributions for w and b\n",
    "    w_dist = Normal(mw_param, sw_param).independent(1)\n",
    "    b_dist = Normal(mb_param, sb_param).independent(1)\n",
    "    dists = {'linear.weight': w_dist, 'linear.bias': b_dist}\n",
    "    # overload the parameters in the module with random samples\n",
    "    # from the guide distributions\n",
    "    lifted_module = pyro.random_module(\"module\", regression_model, dists)\n",
    "    # sample a regressor (which also samples w and b)\n",
    "    return lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam({\"lr\": 0.05})\n",
    "svi = SVI(model, guide, optim, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0001] loss: 1028.0882\n",
      "[iteration 0101] loss: 2.5235\n",
      "[iteration 0201] loss: 0.9560\n",
      "[iteration 0301] loss: -0.1895\n",
      "[iteration 0401] loss: -0.8222\n",
      "[iteration 0501] loss: -1.1446\n",
      "[iteration 0601] loss: -1.1813\n",
      "[iteration 0701] loss: -1.1287\n",
      "[iteration 0801] loss: -1.2544\n",
      "[iteration 0901] loss: -1.1635\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    pyro.clear_param_store()\n",
    "    data = build_linear_dataset(N)\n",
    "    for j in range(num_iterations):\n",
    "        # calculate the loss and take a gradient step\n",
    "        loss = svi.step(data)\n",
    "        if j % 100 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / float(N)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[guide_mean_weight]: 2.992\n",
      "[guide_log_scale_weight]: -3.966\n",
      "[guide_mean_bias]: 1.004\n",
      "[guide_log_scale_bias]: -3.981\n"
     ]
    }
   ],
   "source": [
    "for name in pyro.get_param_store().get_all_param_names():\n",
    "    print(\"[%s]: %.3f\" % (name, pyro.param(name).data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0014413866447284818\n"
     ]
    }
   ],
   "source": [
    "X = np.linspace(6, 7, num=20)\n",
    "y = 3 * X + 1\n",
    "X, y = X.reshape((20, 1)), y.reshape((20, 1))\n",
    "x_data, y_data = torch.tensor(X).type(torch.Tensor), torch.tensor(y).type(torch.Tensor)\n",
    "loss = nn.MSELoss()\n",
    "y_preds = torch.zeros(20, 1)\n",
    "for i in range(20):\n",
    "    # guide does not require the data\n",
    "    sampled_reg_model = guide(None)\n",
    "    # run the regression model and add prediction to total\n",
    "    y_preds = y_preds + sampled_reg_model(x_data)\n",
    "# take the average of the predictions\n",
    "y_preds = y_preds / 20\n",
    "print (\"Loss: \", loss(y_preds, y_data).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
